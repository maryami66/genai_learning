{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Chef: Building a Conversational Recipe Generator with NVIDIA LLM API and LangChain\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to **AI Chef**! This hands-on project will guide you through creating a conversational recipe generator that dynamically tailors recipes based on user preferences, dietary restrictions, and even follow-up instructions. Using **NVIDIA LLM API** and **LangChain**, we’ll progressively build and enhance the model, implementing new features at each step to improve functionality and user experience.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "In this project, you'll construct a recipe generation tool that combines **LangChain** for task structuring with **Streamlit** for an interactive interface. Step by step, we’ll incorporate features like conversation history, contextual prompts, and evaluation metrics, leading to a sophisticated, user-friendly \"AI Chef\" capable of handling multi-turn dialogues and detailed recipe customizations.\n",
    "\n",
    "By the end, you’ll have a robust, conversational \"AI Chef\" capable of creating diverse recipes in response to detailed user inputs. \n",
    "\n",
    "Enjoy your journey with AI Chef, and happy coding!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Connect to the NVIDIA LLM API\n",
    "\n",
    "Establish a connection to the NVIDIA API and send a basic message to confirm it’s working.\n",
    "\n",
    "**Goal**: Set up the NVIDIA API connection and retrieve a basic response. You'll also use `HumanMessage` to structure the prompt, which is how LangChain formats messages for the model.\n",
    "\n",
    "1. **Check the Documentation**  \n",
    "   Review the [ChatNVIDIA documentation](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) to understand the `ChatNVIDIA` class and its `invoke` method. Also, check the [HumanMessage documentation](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) to understand how to structure prompts.\n",
    "\n",
    "2. **Complete the Code Below**  \n",
    "   Fill in the blanks to:\n",
    "   - Set up a connection function (`connect_to_nvidia`).\n",
    "   - Use `HumanMessage` to format a simple prompt for the model.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "You should see the model’s response printed, confirming that the connection and prompt structure are working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here's a recipe for a delicious and unique dish:\n",
      "\n",
      "**Creamy Spinach and Shrimp Tartlets**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "For the crust:\n",
      "\n",
      "* 1 1/2 cups all-purpose flour\n",
      "* 1/4 cup confectioners' sugar\n",
      "* 1/4 cup unsalted butter, chilled and cut into small pieces\n",
      "* 1/4 cup ice water\n",
      "\n",
      "For the filling:\n",
      "\n",
      "* 1/2 cup fresh spinach, chopped\n",
      "* 2 tablespoons unsalted butter\n",
      "* 2 cloves garlic, minced\n",
      "* 1/2 cup heavy cream\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon black pepper\n",
      "* 1/4 teaspoon paprika\n",
      "* 1 cup large shrimp, peeled and deveined\n",
      "* 1 egg, beaten (for egg wash)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Make the crust:** In a food processor, combine flour, confectioners' sugar, and salt. Add the cold butter and process until the mixture resembles coarse crumbs. Gradually add the ice water, pulsing until the dough comes together in a ball. Wrap and refrigerate for at least 30 minutes.\n",
      "2. **Preheat the oven:** Heat the oven to 400°F (200°C). Line a baking sheet with parchment paper.\n",
      "3. **Roll out the crust:** On a lightly floured surface, roll out the chilled dough to a thickness of about 1/8 inch. Cut out 12 equal-sized squares, about 3 inches (7.5 cm) on each side.\n",
      "4. **Prepare the filling:** In a skillet, melt butter over medium heat. Add garlic and cook for 1 minute. Add the chopped spinach and cook until wilted, about 3-4 minutes. Set aside to cool slightly.\n",
      "5. **Assemble the tartlets:** In a deglazing dish, heat the heavy cream over low heat. Stir in Parmesan cheese until melted and smooth. Stir in cooked spinach mixture, paprika, salt, and pepper.\n",
      "6. **Add the shrimp:** In a separate skillet, cook the shrimp over medium heat, stirring occasionally, until pink and cooked through.\n",
      "7. **Assemble the tartlets:** Place a square of dough on a flat surface. Spoon a small amount of the spinach mixture onto one half of the dough square, leaving a 1-inch border around the edges. Arrange the shrimp on top of the spinach mixture.\n",
      "8. **Fold and seal the edges:** Fold the other half of the dough over the filling, pressing the edges to seal. Brush with egg wash and press gently with a fork to seal the edges.\n",
      "9. **Bake the tartlets:** Bake the tartlets for 15-20 minutes, or until golden brown.\n",
      "10. **Serve:** Serve warm, garnished with fresh spinach leaves and a side of lemon wedges, if desired.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* Use frozen spinach, thawed and drained, as a substitute for fresh spinach.\n",
      "* Add other ingredients to the filling, such as diced ham or diced bell peppers, to suit your taste.\n",
      "* Replace the shrimp with diced chicken or turkey, or sliced mushrooms, for a different protein option.\n",
      "\n",
      "Enjoy your delicious Creamy Spinach and Shrimp Tartlets!\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Define API Key and Model Version\n",
    "API_KEY = \"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\"  # Replace with your actual API key\n",
    "MODEL_VERSION = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# Step 0: Connect to NVIDIA API\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API with the specified model.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your NVIDIA API key.\n",
    "        model (str): The model version to use.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance connected to the NVIDIA model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(model=model, api_key=api_key)\n",
    "\n",
    "# Initialize the client\n",
    "client = connect_to_nvidia()\n",
    "\n",
    "# Create a HumanMessage with the prompt\n",
    "prompt = HumanMessage(content=\"Generate a recipe.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify connectivity\n",
    "print(\"Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate a Basic Recipe\n",
    "\n",
    "Now that you have a connection to the NVIDIA API, modify the prompt to include a list of ingredients. This will allow the AI to generate a recipe based on specific inputs.\n",
    "\n",
    "**Goal**: Customize the prompt with a list of ingredients and retrieve a recipe that includes them.\n",
    "\n",
    "1. **Complete the Code Below**  \n",
    "   - Define a list of ingredients.\n",
    "   - Use `HumanMessage` to format the prompt with these ingredients for a tailored recipe.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "The model should respond with a recipe that incorporates the specified ingredients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Response: A classic combination! Here's a simple recipe for you:\n",
      "\n",
      "**Tomato and Basil Bruschetta**\n",
      "\n",
      " Servings: 4-6\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 3-4 ripe tomatoes, diced\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 2 tablespoons olive oil\n",
      "* 4-6 slices of bread ( Ciabatta or Baguette work well)\n",
      "* Salt and pepper to taste\n",
      "* Optional: 1/4 cup freshly grated Parmesan cheese (adds an extra burst of flavor!)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your oven to 400°F (200°C).\n",
      "2. Slice the bread into 1-inch thick slices and toast in the oven for 5-7 minutes, or until lightly browned.\n",
      "3. In a small bowl, whisk together olive oil, salt, and pepper.\n",
      "4. In a separate bowl, combine the diced tomatoes and chopped basil.\n",
      "5. Brush the toasted bread slices with the olive oil mixture.\n",
      "6. Spoon the tomato-basil mixture onto each bread slice, making sure to leave a small border around the edges.\n",
      "7. If using Parmesan cheese, sprinkle a pinch over the top of each slice.\n",
      "8. Serve immediately, and enjoy the flavors of Italy!\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* For added flavor, use a flavored olive oil, such as infused with garlic or herbs.\n",
      "* You can also roast the tomatoes in the oven before using them for a deeper, richer flavor.\n",
      "* Consider using homemade breadcrumbs or olive oil-glazed bread for an extra crunchy texture.\n",
      "* Feel free to add some red onion, garlic, or a pinch of red pepper flakes to give the dish a bit more zing!\n",
      "\n",
      "**Buon appetito!**\n"
     ]
    }
   ],
   "source": [
    "# Define a list of ingredients\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "\n",
    "# Create a prompt that includes the ingredients\n",
    "prompt = HumanMessage(content=f\"Create a recipe using the following ingredients: {', '.join(ingredients)}.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation\n",
    "print(\"Recipe Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Dietary Restrictions\n",
    "\n",
    "Enhance the recipe generation by allowing dietary restrictions. This will enable the AI to create recipes that accommodate specific dietary needs.\n",
    "\n",
    "**Goal**: Modify the prompt to include dietary restrictions, customizing the recipe to align with the specified preferences.\n",
    "\n",
    "1. **Complete the Code Below**  \n",
    "   - Define a list of dietary restrictions.\n",
    "   - Adjust the prompt to include these restrictions.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "The model should respond with a recipe that meets the specified dietary restrictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Response with Dietary Restrictions: Here's a simple and delicious vegan recipe using tomato, basil, and olive oil:\n",
      "\n",
      "**Vegan Tomato Basil Bruschetta**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 2 large ripe tomatoes, diced\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 2 tablespoons olive oil\n",
      "* 2 tablespoons gluten-free breadcrumbs (made from rice, corn, or potato)\n",
      "* Salt and pepper to taste\n",
      "* 1 tablespoon lemon juice (optional)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Preheat your oven to 400°F (200°C).\n",
      "2. In a medium bowl, toss together the diced tomatoes and chopped basil.\n",
      "3. In a small bowl, mix together the olive oil, gluten-free breadcrumbs, salt, and pepper.\n",
      "4. Brush the bread (you can use gluten-free bread or comeeto up with a vegan bread alternative like gluten-free crackers or toasted flatbread) with the olive oil mixture, then sprinkle with the breadcrumb mixture.\n",
      "5. Arrange the bread pieces on a baking sheet and toast in the oven for 5-7 minutes, or until lightly browned.\n",
      "6. Remove the toasted bread from the oven and top each piece with a spoonful of the tomato and basil mixture.\n",
      "7. Drizzle the lemon juice over the top (if using) and serve.\n",
      "\n",
      "**Vegan notes:**\n",
      "\n",
      "* Be sure to choose a vegan-friendly gluten-free breadcrumb option. Many store-bought breadcrumbs contain gluten, so read labels carefully.\n",
      "* If you want to get a bit fancier, you can also top the bruschetta with a dollop of vegan pesto or tapenade.\n",
      "* This recipe is naturally free of animal products, making it a great option for vegans.\n",
      "\n",
      "**Olive oil benefits:**\n",
      "\n",
      "* Olive oil is rich in healthy fats and antioxidants, which can help reduce inflammation and improve heart health.\n",
      "* It's also a rich source of antioxidants, including vitamin E, which can help protect against oxidative stress caused by free radicals.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* For added flavor, you can also roast the tomatoes in the oven with a drizzle of olive oil before adding the basil and breadcrumbs.\n",
      "* If you can't find gluten-free breadcrumbs, you can also make your own by blending gluten-free bread into crumbs in a food processor.\n",
      "* Feel free to get creative with your tomato and basil combination – try pairing it with other herbs like parsley or oregano, or with other vegan cheeses like vegan mozzarella!\n",
      "\n",
      "I hope you enjoy this simple and flavorful recipe!\n"
     ]
    }
   ],
   "source": [
    "# Define a list of ingredients and dietary restrictions\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "\n",
    "# Create a prompt that includes ingredients and dietary restrictions\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\"\n",
    ")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation with dietary restrictions\n",
    "print(\"Recipe Response with Dietary Restrictions:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Customization with Parameters\n",
    "\n",
    "Enhance recipe generation by adding parameters to control the model's creativity and consistency. Adjusting `temperature` and `top_p` allows you to customize the output style and creativity.\n",
    "\n",
    "**Goal**: Use `temperature` and `top_p` parameters to customize the AI’s response style for recipe generation.\n",
    "\n",
    "1. **Review the Parameter Effects**  \n",
    "   - `temperature`: Controls randomness in the response (higher values make the response more creative, while lower values make it more focused).\n",
    "   - `top_p`: Controls the diversity of words used (higher values allow for more diverse outputs).\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   - Modify `connect_to_nvidia` to accept `temperature` and `top_p` as arguments.\n",
    "   - Use these parameters when creating the `ChatNVIDIA` instance.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this code should produce recipes that vary in style or tone depending on the values of `temperature` and `top_p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Response: What a lovely combination! Here's a simple and delicious recipe that incorporates all three ingredients:\n",
      "\n",
      "**Vegan Tomato and Basil Bruschetta**\n",
      "\n",
      "**Servings:** 4-6\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 2 large tomatoes, diced\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 1/4 cup olive oil\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon black pepper\n",
      "* 4-6 gluten-free bread slices (made from rice, corn, or almond flour)\n",
      "* Optional: 1/4 cup vegan mozzarella shreds (such as Daiya or Follow Your Heart)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Preheat your oven to 400°F (200°C).\n",
      "2. Slice the gluten-free bread into 1/2-inch thick slices and toast in the oven for 5-7 minutes, or until lightly browned.\n",
      "3. In a medium bowl, combine the diced tomatoes, chopped basil, salt, and black pepper. Drizzle with olive oil and toss to coat.\n",
      "4. Once the bread is toasted, let it cool for a minute or two. Then, rub the garlic cloves (if using) onto each bread slice, followed by spooning the tomato-basil mixture onto each slice.\n",
      "5. If using vegan mozzarella shreds, sprinkle them on top of the tomato mixture.\n",
      "6. Serve immediately and enjoy!\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* For an extra burst of flavor, drizzle a little balsamic vinegar or lemon juice over the tomato mixture.\n",
      "* If you prefer a more intense basil flavor, you can add a few sprigs of fresh basil to the tomato mixture and let it sit for 10-15 minutes to allow the flavors to meld.\n",
      "* Consider using other gluten-free bread options, such as gluten-free baguette slices or crostini.\n",
      "\n",
      "This recipe is not only delicious, but it's also vegan and gluten-free, making it perfect for those with dietary restrictions. The olive oil and basil add a rich and herbaceous flavor, while the tomatoes provide a burst of juicy sweetness. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "# Update the connect_to_nvidia function to accept temperature and top_p\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = 0.5, top_p: float = 0.7) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API with configurable temperature and top_p parameters.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your NVIDIA API key.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Controls creativity in the response.\n",
    "        top_p (float): Controls the diversity of words used.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance connected to the NVIDIA model with specified parameters.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(model=model, api_key=api_key, temperature=temperature, top_p=top_p, max_tokens=1024)\n",
    "\n",
    "# Initialize the client with custom temperature and top_p values\n",
    "temperature = 0.5 \n",
    "top_p = 0.7\n",
    "client = connect_to_nvidia(temperature=temperature, top_p=top_p)\n",
    "\n",
    "# Define a list of ingredients and dietary restrictions\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "\n",
    "# Create a prompt with the ingredients and dietary restrictions\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation\n",
    "print(\"Recipe Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Different Course Types\n",
    "\n",
    "Add a parameter for the type of course (e.g., \"main dish\", \"dessert\", \"appetizer\") to make the recipe more specific. This will help generate recipes tailored to the selected course type.\n",
    "\n",
    "**Goal**: Modify the prompt to include a `course_type` parameter, which allows the model to generate recipes for different types of meals.\n",
    "\n",
    "1. **Define the Course Type**  \n",
    "   Add a variable for the course type, which could be \"main dish\", \"dessert\", or \"appetizer\".\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   - Include `course_type` in the prompt along with ingredients and dietary restrictions.\n",
    "   \n",
    "**Expected Outcome**:  \n",
    "Running this code should produce recipes that are tailored to the specified course type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Response for Course Type: Here's a delicious vegan and gluten-free main dish recipe that incorporates the ingredients you mentioned:\n",
      "\n",
      "**Vegan Tomato and Basil Pasta with Olive Oil Sauce**\n",
      "\n",
      "** Servings: 4-6**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 8 oz. gluten-free pasta (made from rice, quinoa, or corn)\n",
      "* 2 large tomatoes, diced\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 1/4 cup olive oil\n",
      "* 2 cloves garlic, minced (optional)\n",
      "* Salt and pepper, to taste\n",
      "* Fresh basil leaves, for garnish\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Cook the pasta**: Bring a large pot of salted water to a boil. Cook the gluten-free pasta according to the package instructions until al dente. Reserve 1 cup of pasta water before draining.\n",
      "2. **Make the sauce**: In a blender or food processor, combine the diced tomatoes, chopped basil, garlic (if using), and 2 tablespoons of olive oil. Blend until smooth.\n",
      "3. **Heat the sauce**: In a large skillet, heat the remaining 2 tablespoons of olive oil over medium heat. Add the tomato-basil sauce and stir to combine. Bring the sauce to a simmer and let cook for 5-7 minutes, stirring occasionally, until the sauce has thickened slightly.\n",
      "4. **Combine pasta and sauce**: Add the cooked pasta to the skillet with the tomato-basil sauce. Toss to combine, adding some reserved pasta water if the sauce seems too thick.\n",
      "5. **Season and serve**: Season with salt and pepper to taste. Garnish with fresh basil leaves and serve hot.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* For an extra burst of flavor, add some lemon juice or zest to the sauce.\n",
      "* If you prefer a creamier sauce, you can add a tablespoon or two of vegan cream or nutritional yeast.\n",
      "* Feel free to customize the recipe by adding other vegetables, such as bell peppers or mushrooms, to the sauce.\n",
      "* For a more substantial meal, add some roasted vegetables or a side salad to the dish.\n",
      "\n",
      "Enjoy your delicious and satisfying vegan and gluten-free main dish!\n"
     ]
    }
   ],
   "source": [
    "# Define a list of ingredients, dietary restrictions, and course type\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "course_type = \"main dish\"  # Example: \"main dish\"\n",
    "\n",
    "# Create a prompt that includes ingredients, dietary restrictions, and course type\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a {course_type} recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\"\n",
    ")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation for the specified course type\n",
    "print(\"Recipe Response for Course Type:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation\n",
    "\n",
    "Introduce evaluation metrics to assess the quality and relevance of the generated recipes. We’ll calculate:\n",
    "- **Ingredient Coverage**: Measures how many specified ingredients appear in the recipe.\n",
    "- **Lexical Diversity**: Measures the variety of words used.\n",
    "- **Readability**: Measures how easy the recipe is to read.\n",
    "\n",
    "**Goal**: Implement functions to evaluate the quality of generated recipes based on these metrics.\n",
    "\n",
    "1. **Implement Scoring Functions**  \n",
    "   - `ingredient_coverage_score`: Checks how many ingredients from the list appear in the recipe.\n",
    "   - `lexical_diversity_score`: Calculates the variety of words in the recipe.\n",
    "   - `readability_score`: Calculates the readability of the recipe text.\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   Implement these functions and use them to evaluate the response generated by the model.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this code should print the evaluation scores for ingredient coverage, lexical diversity, and readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: Here's a delicious vegan and gluten-free main dish recipe that incorporates the ingredients you mentioned:\n",
      "\n",
      "**Vegan Tomato and Basil Pasta with Olive Oil Sauce**\n",
      "\n",
      "** Servings: 4-6**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 8 oz. gluten-free pasta (made from rice, quinoa, or corn)\n",
      "* 2 large tomatoes, diced\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 1/4 cup olive oil\n",
      "* 2 cloves garlic, minced (optional)\n",
      "* Salt and pepper, to taste\n",
      "* Fresh basil leaves, for garnish\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Cook the pasta**: Bring a large pot of salted water to a boil. Cook the gluten-free pasta according to the package instructions until al dente. Reserve 1 cup of pasta water before draining.\n",
      "2. **Make the sauce**: In a blender or food processor, combine the diced tomatoes, chopped basil, garlic (if using), and 2 tablespoons of olive oil. Blend until smooth.\n",
      "3. **Heat the sauce**: In a large skillet, heat the remaining 2 tablespoons of olive oil over medium heat. Add the tomato-basil sauce and stir to combine. Bring the sauce to a simmer and let cook for 5-7 minutes, stirring occasionally, until the sauce has thickened slightly.\n",
      "4. **Combine pasta and sauce**: Add the cooked pasta to the skillet with the tomato-basil sauce. Toss to combine, adding some reserved pasta water if the sauce seems too thick.\n",
      "5. **Season and serve**: Season with salt and pepper to taste. Garnish with fresh basil leaves and serve hot.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* For an extra burst of flavor, add some lemon juice or zest to the sauce.\n",
      "* If you prefer a creamier sauce, you can add a tablespoon or two of vegan cream or nutritional yeast.\n",
      "* Feel free to customize the recipe by adding other vegetables, such as bell peppers or mushrooms, to the sauce.\n",
      "* For a more substantial meal, add some roasted vegetables or a side salad to the dish.\n",
      "\n",
      "Enjoy your delicious and satisfying vegan and gluten-free main dish!\n",
      "Ingredient Coverage Score: 1.0\n",
      "Lexical Diversity Score: 0.5924764890282131\n",
      "Readability Score: 71.95\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "# Define function to calculate ingredient coverage score\n",
    "def ingredient_coverage_score(recipe: str, ingredients: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the proportion of specified ingredients found in the recipe.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "        ingredients (list): List of ingredients provided in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of specified ingredients found in the recipe text.\n",
    "               Value ranges from 0 to 1, where 1 means all ingredients are present.\n",
    "    \"\"\"\n",
    "    # Convert recipe to lowercase to ensure case-insensitive matching\n",
    "    recipe_lower = recipe.lower()\n",
    "    \n",
    "    # Count how many ingredients appear in the recipe\n",
    "    ingredient_matches = sum(1 for ingredient in ingredients if ingredient.lower() in recipe_lower)\n",
    "    \n",
    "    # Calculate and return the coverage score\n",
    "    return ingredient_matches / len(ingredients) if ingredients else 0\n",
    "\n",
    "# Define function to calculate lexical diversity score\n",
    "def lexical_diversity_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical diversity of the recipe text.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Lexical diversity score, which is the ratio of unique words\n",
    "               to total words. A higher score indicates greater word variety.\n",
    "    \"\"\"\n",
    "    # Split the recipe into words\n",
    "    words = recipe.split()\n",
    "    \n",
    "    # Identify unique words and calculate diversity\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "# Define function to calculate readability score\n",
    "def readability_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the readability score of the recipe using Flesch Reading Ease.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Readability score; higher values indicate easier-to-read text.\n",
    "    \"\"\"\n",
    "    return textstat.flesch_reading_ease(recipe)\n",
    "\n",
    "# Generate a recipe as in previous steps\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "course_type = \"main dish\"\n",
    "\n",
    "# Create the prompt with ingredients, dietary restrictions, and course type\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a {course_type} recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\"\n",
    ")\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Extract recipe text from the model's response\n",
    "recipe_text = response.content\n",
    "\n",
    "# Print the generated recipe\n",
    "print(\"Recipe:\", recipe_text)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "ic_score = ingredient_coverage_score(recipe_text, ingredients)\n",
    "ld_score = lexical_diversity_score(recipe_text)\n",
    "r_score = readability_score(recipe_text)\n",
    "print(\"Ingredient Coverage Score:\", ic_score)  # Measures ingredient inclusion\n",
    "print(\"Lexical Diversity Score:\", ld_score)  # Indicates word variety in the recipe\n",
    "print(\"Readability Score:\", r_score)  # Measures ease of reading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Logging to Track Model Interactions and Evaluation Metrics\n",
    "\n",
    "In this step, we’ll introduce logging to keep track of each model interaction, including the prompt, response, model parameters (`temperature` and `top_p`), and evaluation metrics. Logging helps in monitoring, debugging, and analyzing the model's behavior over time.\n",
    "\n",
    "**Goal**: Set up logging to save interaction details for every generated recipe.\n",
    "\n",
    "1. **Configure Logging**  \n",
    "   Set up a basic logging configuration with a file named `llm_logs.log`. Configure it to log messages with timestamps, which will be useful for tracking when each interaction occurred.\n",
    "\n",
    "2. **Define a Logging Function**  \n",
    "   Create a function called `log_llm_interaction` that takes the following parameters:\n",
    "   - `prompt`: The input prompt sent to the LLM.\n",
    "   - `response`: The recipe generated by the LLM.\n",
    "   - `temperature` and `top_p`: Model parameters used for sampling.\n",
    "   - `coverage_score`, `diversity_score`, and `readability`: Evaluation metrics for the generated recipe.\n",
    "\n",
    "   Within this function, use `logging.info` to log each piece of information in a structured format.\n",
    "\n",
    "3. **Log the Interaction**  \n",
    "   After generating and evaluating a recipe, call the `log_llm_interaction` function with the prompt, response, model parameters, and evaluation scores.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this step should save a log entry in `llm_logs.log` with the prompt, model response, parameters, and evaluation metrics for each interaction, enabling you to analyze and troubleshoot model performance effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define function to log LLM interactions, including model parameters and evaluation metrics\n",
    "def log_llm_interaction(prompt: str, response: str, temperature: float, top_p: float, coverage_score: float, diversity_score: float, readability: float):\n",
    "    \"\"\"\n",
    "    Logs each interaction with the LLM, including the prompt, response, model parameters, and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the LLM.\n",
    "        response (str): The LLM's response.\n",
    "        temperature (float): Model's temperature parameter used for this interaction.\n",
    "        top_p (float): Model's top_p parameter used for this interaction.\n",
    "        coverage_score (float): Ingredient coverage score.\n",
    "        diversity_score (float): Lexical diversity score.\n",
    "        readability (float): Readability score of the response.\n",
    "    \"\"\"\n",
    "    logging.info(\"Prompt: %s\", prompt)\n",
    "    logging.info(\"Model Parameters: Temperature=%.2f, Top_p=%.2f\", temperature, top_p)\n",
    "    logging.info(\"Evaluation Scores: Coverage=%.2f, Diversity=%.2f, Readability=%.2f\", coverage_score, diversity_score, readability)\n",
    "    logging.info(\"Response: %s\", response)\n",
    "\n",
    "# Log interaction and evaluation metrics after recipe generation and evaluation\n",
    "log_llm_interaction(prompt.content, recipe_text, temperature, top_p, ic_score, ld_score, r_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Add Streamlit Interface with Logging\n",
    "\n",
    "In this step, we’ll set up a Streamlit interface for the **AI Chef** recipe generator, allowing users to input recipe preferences and generate recipes with the click of a button. This code also integrates logging to track interactions for analysis and debugging.\n",
    "\n",
    "**Goal**: Create a basic Streamlit interface where users can input recipe parameters, click a \"Generate Recipe\" button, and view the generated recipe.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Copy & Paste**: Copy this code into a new Python file, e.g., `ai_chef_streamlit.py`.\n",
    "2. **Fill in the Blanks**:\n",
    "   - Complete the blanks below to set up the API connection, input fields, and button functionality.\n",
    "3. **Activate Virtual Environment and Install Dependencies**:\n",
    "   - Ensure you have a virtual environment set up and activated.\n",
    "     ```bash\n",
    "     source .venv/bin/activate  # MacOS/Linux\n",
    "     .venv\\Scripts\\activate     # Windows\n",
    "     ```\n",
    "   - Ensure you have all necessary libraries installed, using `requirements.txt`:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "   - If you need to create a `requirements.txt` file, it should include the following:\n",
    "     ```plaintext\n",
    "     streamlit\n",
    "     textstat\n",
    "     langchain_nvidia_ai_endpoints\n",
    "     ```\n",
    "4. **Run the Application**:\n",
    "   - From the terminal, navigate to the directory containing `ai_chef_streamlit.py` and run:\n",
    "     ```bash\n",
    "     streamlit run ai_chef_streamlit.py\n",
    "     ```\n",
    "   - This will open the application in your default browser, enabling you to interact with the recipe generator via the Streamlit interface.\n",
    "\n",
    "**Expected Outcome**: A functional Streamlit app where you can input recipe parameters, click \"Generate Recipe,\" and view the result with evaluation scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import List, Optional\n",
    "import textstat\n",
    "\n",
    "# Set up logging configuration to save interactions in llm_logs.log with timestamps\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define API key and model version (replace 'your_actual_api_key' with your API key)\n",
    "API_KEY = \"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\"  \n",
    "MODEL_VERSION = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# Connect to NVIDIA API using ChatNVIDIA with specific parameters\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = 0.5, top_p: float = 0.7) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): The API key for NVIDIA access.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Sampling temperature for creativity.\n",
    "        top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance of ChatNVIDIA connected to the specified model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "# Define function to calculate ingredient coverage score\n",
    "def ingredient_coverage_score(recipe: str, ingredients: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the proportion of specified ingredients found in the recipe.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "        ingredients (list): List of ingredients provided in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of specified ingredients found in the recipe text,\n",
    "               ranging from 0 to 1, where 1 means all ingredients are present.\n",
    "    \"\"\"\n",
    "    recipe_lower = recipe.lower()\n",
    "    ingredient_matches = sum(1 for ingredient in ingredients if ingredient.lower() in recipe_lower)\n",
    "    return ingredient_matches / len(ingredients) if ingredients else 0\n",
    "\n",
    "# Define function to calculate lexical diversity score\n",
    "def lexical_diversity_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical diversity of the recipe text.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Lexical diversity score, which is the ratio of unique words\n",
    "               to total words. A higher score indicates greater word variety.\n",
    "    \"\"\"\n",
    "    words = recipe.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "# Define function to calculate readability score\n",
    "def readability_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the readability score of the recipe using Flesch Reading Ease.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Readability score; higher values indicate easier-to-read text.\n",
    "    \"\"\"\n",
    "    return textstat.flesch_reading_ease(recipe)\n",
    "\n",
    "# Define function to log LLM interactions, including model parameters and evaluation metrics\n",
    "def log_llm_interaction(prompt: str, response: str, temperature: float, top_p: float, coverage_score: float, diversity_score: float, readability: float):\n",
    "    \"\"\"\n",
    "    Logs each interaction with the LLM, including the prompt, response, model parameters, and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the LLM.\n",
    "        response (str): The LLM's response.\n",
    "        temperature (float): Model's temperature parameter used for this interaction.\n",
    "        top_p (float): Model's top_p parameter used for this interaction.\n",
    "        coverage_score (float): Ingredient coverage score.\n",
    "        diversity_score (float): Lexical diversity score.\n",
    "        readability (float): Readability score of the response.\n",
    "    \"\"\"\n",
    "    logging.info(\"Prompt: %s\", prompt)\n",
    "    logging.info(\"Model Parameters: Temperature=%.2f, Top_p=%.2f\", temperature, top_p)\n",
    "    logging.info(\"Evaluation Scores: Coverage=%.2f, Diversity=%.2f, Readability=%.2f\", coverage_score, diversity_score, readability)\n",
    "    logging.info(\"Response: %s\", response)\n",
    "\n",
    "# Main recipe generation function\n",
    "def generate_recipe(client, ingredients: List[str], dietary_restrictions: Optional[List[str]] = None, \n",
    "                    course_type: str = \"main dish\", preference: str = \"easy\", additional_request: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a recipe from the NVIDIA LLM model based on a structured prompt.\n",
    "\n",
    "    Args:\n",
    "        client (ChatNVIDIA): The LLM client instance.\n",
    "        ingredients (List[str]): List of ingredients for the recipe.\n",
    "        dietary_restrictions (Optional[List[str]]): Any dietary restrictions.\n",
    "        course_type (str): Type of course (e.g., main dish).\n",
    "        preference (str): User's recipe preference (e.g., easy).\n",
    "        additional_request (str): Additional user input or request.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated recipe text along with evaluation scores.\n",
    "    \"\"\"\n",
    "    # Build prompt\n",
    "    prompt_content = f\"Create a {preference} recipe for a {course_type} using the following ingredients: {', '.join(ingredients)}. \" \\\n",
    "                     f\"Ensure the recipe is {', '.join(dietary_restrictions) if dietary_restrictions else 'no specific dietary restrictions'}. {additional_request}\"\n",
    "    prompt = HumanMessage(content=prompt_content)\n",
    "    \n",
    "    # Send prompt to the model and get response\n",
    "    response = client.invoke([prompt])\n",
    "    recipe_text = response.content\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    coverage_score = ingredient_coverage_score(recipe_text, ingredients)\n",
    "    diversity_score = lexical_diversity_score(recipe_text)\n",
    "    readability = readability_score(recipe_text)\n",
    "\n",
    "    # Log interaction and evaluation metrics\n",
    "    log_llm_interaction(prompt_content, recipe_text, client.temperature, client.top_p, coverage_score, diversity_score, readability)\n",
    "    \n",
    "    return recipe_text, coverage_score, diversity_score, readability\n",
    "\n",
    "# Streamlit UI setup\n",
    "st.title(\"AI Chef: Your Personalized Recipe Generator\")\n",
    "\n",
    "# Input fields for recipe customization\n",
    "ingredients = st.text_input(\"Enter ingredients (comma-separated):\").split(\", \")\n",
    "dietary_restrictions = st.multiselect(\"Select dietary restrictions\", [\"gluten-free\", \"vegan\", \"vegetarian\", \"dairy-free\"])\n",
    "course_type = st.selectbox(\"Select course type\", [\"Main dish\", \"Dessert\", \"Appetizer\"])\n",
    "preference = st.selectbox(\"Select preference\", [\"easy\", \"gourmet\", \"quick\"])\n",
    "temperature = st.slider(\"Select creativity level (temperature)\", 0.0, 1.0, 0.5)\n",
    "top_p = st.slider(\"Select top_p sampling\", 0.0, 1.0, 0.7)\n",
    "additional_request = st.text_input(\"Any additional requests?\")\n",
    "\n",
    "# Button to generate recipe\n",
    "if st.button(\"Recipe\"):\n",
    "    # Connect to NVIDIA client with specified temperature and top_p\n",
    "    client = connect_to_nvidia(temperature=temperature, top_p=top_p)\n",
    "    \n",
    "    # Generate recipe based on user inputs and log the interaction\n",
    "    recipe_text, coverage_score, diversity_score, readability = generate_recipe(\n",
    "        client, ingredients, dietary_restrictions, course_type, preference, additional_request\n",
    "    )\n",
    "\n",
    "    # Display generated recipe and evaluation scores\n",
    "    st.write(\"### Generated Recipe:\")\n",
    "    st.write(recipe_text)\n",
    "    st.write(\"### Evaluation Scores:\")\n",
    "    st.write(f\"Ingredient Coverage Score: {coverage_score:.2f} (0 to 1 scale)\")\n",
    "    st.write(f\"Lexical Diversity Score: {diversity_score:.2f} (0 to 1 scale)\")\n",
    "    st.write(f\"Readability Score (Flesch Reading Ease): {readability:.2f} (0 to 100 scale)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Add Conversation History with Chat Context\n",
    "\n",
    "In this step, we’ll enhance the **AI Chef** application by adding a conversational memory, so the model can remember prior messages and respond with more contextually relevant recipes. We’ll also display the chat history in the Streamlit interface for a fully interactive experience.\n",
    "\n",
    "**Goal**: Enable the model to generate recipes based on conversation history, allowing for follow-up requests or modifications to previous recipes in a conversational flow.\n",
    "\n",
    "### Key Enhancements\n",
    "\n",
    "1. **Conversation History with `recipe_history`**:\n",
    "   - We initialize a `recipe_history` list in `st.session_state` to store the user and AI messages.\n",
    "   - Using this, each exchange is stored, creating a continuous conversation history.\n",
    "   - **Documentation**: [Streamlit Session State](https://docs.streamlit.io/library/api-reference/session-state)\n",
    "\n",
    "2. **Using `ChatPromptTemplate` and `MessagesPlaceholder` for Contextual Responses**:\n",
    "   - We use `ChatPromptTemplate` and `MessagesPlaceholder` from `langchain_core.prompts` to allow the prompt to dynamically include the conversation history.\n",
    "   - `MessagesPlaceholder` works as a template that populates with messages from `recipe_history` before each interaction.\n",
    "   - **Documentation**:\n",
    "     - [ChatPromptTemplate](https://langchain-langchain.readthedocs-hosted.com/en/latest/reference/modules/prompts.html)\n",
    "     - [MessagesPlaceholder](https://langchain-langchain.readthedocs-hosted.com/en/latest/reference/modules/messages.html)\n",
    "\n",
    "3. **Displaying Conversation History in Streamlit**:\n",
    "   - A loop iterates over `recipe_history` to display the full chat history in a conversational format.\n",
    "   - Using `st.chat_message()` function, we format messages distinctly for user and AI, creating a chat-like UI in Streamlit.\n",
    "   - **Documentation**: [st.chat_message](https://docs.streamlit.io/library/api-reference/widgets/st.chat_message)\n",
    "\n",
    "4. **Capturing User Input with Context Awareness**:\n",
    "   - We use `st.chat_input` to capture user inputs and automatically append them to the chat history.\n",
    "   - Each time the user submits a new input, the `generate_recipe` function reads the full conversation from `recipe_history` for context-aware response generation.\n",
    "   - **Documentation**: [st.chat_input](https://docs.streamlit.io/library/api-reference/widgets/st.chat_input)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Copy & Paste**:\n",
    "   - Copy this code into your existing Python file or a new one (e.g., `ai_chef_chat.py`).\n",
    "\n",
    "2. **Fill in Blanks**:\n",
    "   - Complete any blanks to integrate API connection, chat template, and conversation display.\n",
    "\n",
    "3. **Activate Virtual Environment and Install Dependencies**:\n",
    "   - Activate your virtual environment:\n",
    "     ```bash\n",
    "     source .venv/bin/activate  # MacOS/Linux\n",
    "     .venv\\Scripts\\activate     # Windows\n",
    "     ```\n",
    "   - Install dependencies from `requirements.txt`:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "\n",
    "4. **Run the Application**:\n",
    "   - From your terminal, navigate to the file’s directory and run:\n",
    "     ```bash\n",
    "     streamlit run ai_chef_chat.py\n",
    "     ```\n",
    "   - Open the Streamlit app in your browser to interact with the conversation-enabled recipe generator.\n",
    "\n",
    "**Expected Outcome**: An enhanced Streamlit app where you can hold a conversation with the AI Chef, building recipes interactively with follow-up queries and a displayed chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing import List, Optional\n",
    "import textstat\n",
    "\n",
    "# Set up logging configuration to save interactions in llm_logs.log with timestamps\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define API key and model version (replace 'your_actual_api_key' with your API key)\n",
    "API_KEY = \"your_actual_api_key\"\n",
    "MODEL_VERSION = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# Initialize conversation memory and recipe history in Streamlit session\n",
    "if 'recipe_history' not in st.session_state:\n",
    "    st.session_state.recipe_history = []  # List to store user and AI messages for chat history\n",
    "\n",
    "# Connect to NVIDIA API using ChatNVIDIA with specified parameters\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = 0.5, top_p: float = 0.7) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): The API key for NVIDIA access.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Sampling temperature for creativity.\n",
    "        top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance of ChatNVIDIA connected to the specified model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "# Function to log LLM interactions, including model parameters and evaluation metrics\n",
    "def log_llm_interaction(prompt: str, response: str, temperature: float, top_p: float, coverage_score: float, diversity_score: float, readability: float):\n",
    "    \"\"\"\n",
    "    Logs each interaction with the LLM, including the prompt, response, model parameters, and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the LLM.\n",
    "        response (str): The LLM's response.\n",
    "        temperature (float): Model's temperature parameter used for this interaction.\n",
    "        top_p (float): Model's top_p parameter used for this interaction.\n",
    "        coverage_score (float): Ingredient coverage score.\n",
    "        diversity_score (float): Lexical diversity score.\n",
    "        readability (float): Readability score of the response.\n",
    "    \"\"\"\n",
    "    logging.info(\"Prompt: %s\", prompt)\n",
    "    logging.info(\"Model Parameters: Temperature=%.2f, Top_p=%.2f\", temperature, top_p)\n",
    "    logging.info(\"Response: %s\", response)\n",
    "    logging.info(\"Coverage: %.2f, Diversity: %.2f, Readability: %.2f\", coverage_score, diversity_score, readability)\n",
    "\n",
    "# Main recipe generation function with conversation history\n",
    "def generate_recipe(client, ingredients: List[str], dietary_restrictions: Optional[List[str]] = None, \n",
    "                    course_type: str = \"main dish\", preference: str = \"easy\", additional_request: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a recipe from the NVIDIA LLM model based on input and conversation history.\n",
    "\n",
    "    Args:\n",
    "        client (ChatNVIDIA): The LLM client instance.\n",
    "        ingredients (List[str]): List of ingredients for the recipe.\n",
    "        dietary_restrictions (Optional[List[str]]): Any dietary restrictions.\n",
    "        course_type (str): Type of course (e.g., main dish).\n",
    "        preference (str): User's recipe preference (e.g., easy).\n",
    "        additional_request (str): Additional user input or request.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated recipe text along with evaluation scores.\n",
    "    \"\"\"\n",
    "    # Prepare conversation history\n",
    "    conversation_history = [\n",
    "        HumanMessage(content=msg['content']) if msg['role'] == \"user\" else SystemMessage(content=msg['content'])\n",
    "        for msg in st.session_state.recipe_history\n",
    "    ]\n",
    "\n",
    "    # Build prompt with conversation history\n",
    "    prompt_content = f\"Create a {preference} recipe for a {course_type} using these ingredients: {', '.join(ingredients)}. \"\n",
    "    prompt_content += f\"Ensure the recipe is {', '.join(dietary_restrictions) if dietary_restrictions else 'no specific dietary restrictions'}. {additional_request}\"\n",
    "    conversation_history.append(HumanMessage(content=prompt_content))\n",
    "\n",
    "    # Set up ChatPromptTemplate with conversation history\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful AI Chef assistant creating personalized recipes.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    chain = prompt_template | client\n",
    "    ai_response = chain.invoke({\"messages\": conversation_history}).content  # Get response content\n",
    "\n",
    "    # Save AI response to recipe history\n",
    "    st.session_state.recipe_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # Evaluate and log the recipe\n",
    "    coverage_score = ingredient_coverage_score(ai_response, ingredients)\n",
    "    diversity_score = lexical_diversity_score(ai_response)\n",
    "    readability = readability_score(ai_response)\n",
    "    log_llm_interaction(prompt_content, ai_response, client.temperature, client.top_p, coverage_score, diversity_score, readability)\n",
    "\n",
    "    return ai_response, coverage_score, diversity_score, readability\n",
    "\n",
    "# Define evaluation functions\n",
    "def ingredient_coverage_score(recipe: str, ingredients: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the ingredient coverage score based on the number of ingredients used.\n",
    "\n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "        ingredients (List[str]): List of ingredients provided in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of specified ingredients found in the recipe text.\n",
    "    \"\"\"\n",
    "    recipe_lower = recipe.lower()\n",
    "    matches = sum(1 for ingredient in ingredients if ingredient.lower() in recipe_lower)\n",
    "    return matches / len(ingredients) if ingredients else 0\n",
    "\n",
    "def lexical_diversity_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical diversity of the recipe text.\n",
    "\n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Lexical diversity score, indicating word variety.\n",
    "    \"\"\"\n",
    "    words = recipe.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "def readability_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the readability score of the recipe using Flesch Reading Ease.\n",
    "\n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Readability score (higher values indicate easier readability).\n",
    "    \"\"\"\n",
    "    return textstat.flesch_reading_ease(recipe)\n",
    "\n",
    "# Streamlit UI setup\n",
    "st.title(\"AI Chef: Your Personalized Recipe Generator\")\n",
    "\n",
    "# Input fields for recipe customization\n",
    "ingredients = st.text_input(\"Enter ingredients (comma-separated):\").split(\", \")\n",
    "dietary_restrictions = st.multiselect(\"Select dietary restrictions\", [\"gluten-free\", \"vegan\", \"vegetarian\", \"dairy-free\"])\n",
    "course_type = st.selectbox(\"Select course type\", [\"Main dish\", \"Dessert\", \"Appetizer\"])\n",
    "preference = st.selectbox(\"Select preference\", [\"easy\", \"gourmet\", \"quick\"])\n",
    "temperature = st.slider(\"Select creativity level (temperature)\", 0.0, 1.0, 0.5)\n",
    "top_p = st.slider(\"Select top_p sampling\", 0.0, 1.0, 0.7)\n",
    "\n",
    "# Initialize ChatNVIDIA client with configured temperature and top_p\n",
    "if \"client\" not in st.session_state:\n",
    "    st.session_state.client = connect_to_nvidia(temperature=temperature, top_p=top_p)\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.recipe_history:\n",
    "    if message[\"role\"] == \"user\":\n",
    "        st.chat_message(\"user\").markdown(f\"**You:** {message['content']}\")\n",
    "    else:\n",
    "        st.chat_message(\"assistant\").markdown(f\"**AI Chef:** {message['content']}\")\n",
    "\n",
    "# User input for additional requests\n",
    "user_input = st.chat_input(\"Enter additional requests or adjustments:\")\n",
    "if user_input:\n",
    "    st.session_state.recipe_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    st.chat_message(\"user\").markdown(f\"**You:** {user_input}\")\n",
    "\n",
    "    # Generate recipe with conversation context\n",
    "    with st.spinner():\n",
    "        recipe_text, coverage_score, diversity_score, readability = generate_recipe(\n",
    "            st.session_state.client, ingredients, dietary_restrictions, course_type, preference, user_input\n",
    "        )\n",
    "        st.chat_message(\"assistant\").markdown(f\"**AI Chef:** {recipe_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Free Exploration and Advanced Extensions\n",
    "\n",
    "Congratulations on implementing a robust, interactive AI Chef tool! Now that you've mastered recipe history, multi-turn conversations, and memory for user preferences, it’s time to explore even further. This step is designed for **free exploration**, giving you the flexibility to enhance AI Chef in innovative ways, experiment with new GenAI concepts, or even apply your learning to a fresh project.\n",
    "\n",
    "Here are a few **advanced ideas and resources** to consider for your exploration:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Implement Autonomous Task Management with Agents\n",
    "\n",
    "Introduce agent capabilities, allowing AI Chef to autonomously handle complex workflows or multi-step user requests. For instance, you could program an agent to proactively suggest recipes based on seasonal ingredients or past user behavior.\n",
    "\n",
    "#### Suggested Docs:\n",
    "- [LangGraph Overview](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-1-build-a-basic-chatbot)\n",
    "\n",
    "### 2. Apply the AI Chef Framework to New Use Cases\n",
    "\n",
    "Replicate the AI Chef model to other domains, such as an **AI Travel Planner** for itinerary suggestions or an **AI Health Coach** for personalized wellness plans. Translating the functionality to a new use case will highlight GenAI’s versatility.\n",
    "\n",
    "#### Example Use Cases:\n",
    "- AI Travel Planner: Generates travel itineraries with hotels, sights, and tips.\n",
    "- AI Health Coach: Offers personalized health and fitness plans.\n",
    "\n",
    "### 3. Refine and Optimize AI Chef\n",
    "\n",
    "Enhance the existing AI Chef tool with more advanced features and interactivity.\n",
    "\n",
    "- Check out more about [Streamlit](https://docs.streamlit.io/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "These are just some of the ways to deepen your understanding and extend AI Chef’s functionality. Enjoy exploring these advanced directions, and happy coding!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
