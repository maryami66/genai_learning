{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Chef: Building a Conversational Recipe Generator with NVIDIA LLM API and LangChain\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to **AI Chef**! This hands-on project will guide you through creating a conversational recipe generator that dynamically tailors recipes based on user preferences, dietary restrictions, and even follow-up instructions. Using **NVIDIA LLM API** and **LangChain**, we’ll progressively build and enhance the model, implementing new features at each step to improve functionality and user experience.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "In this project, you'll construct a recipe generation tool that combines **LangChain** for task structuring with **Streamlit** for an interactive interface. Step by step, we’ll incorporate features like conversation history, contextual prompts, and evaluation metrics, leading to a sophisticated, user-friendly \"AI Chef\" capable of handling multi-turn dialogues and detailed recipe customizations.\n",
    "\n",
    "By the end, you’ll have a robust, conversational \"AI Chef\" capable of creating diverse recipes in response to detailed user inputs. \n",
    "\n",
    "Enjoy your journey with AI Chef, and happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Connect to the NVIDIA LLM API\n",
    "\n",
    "Establish a connection to the NVIDIA API and send a basic message to confirm it’s working.\n",
    "\n",
    "**Goal**: Set up the NVIDIA API connection and retrieve a basic response. You'll also use `HumanMessage` to structure the prompt, which is how LangChain formats messages for the model.\n",
    "\n",
    "1. **Check the Documentation**  \n",
    "   Review the [ChatNVIDIA documentation](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) to understand the `ChatNVIDIA` class and its `invoke` method. Also, check the [HumanMessage documentation](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) to understand how to structure prompts.\n",
    "\n",
    "2. **Complete the Code Below**  \n",
    "   Fill in the blanks to:\n",
    "   - Set up a connection function (`connect_to_nvidia`).\n",
    "   - Use `HumanMessage` to format a simple prompt for the model.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "You should see the model’s response printed, confirming that the connection and prompt structure are working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here's a recipe for a delicious and unique dish:\n",
      "\n",
      "**Creamy Spinach and Shrimp Tartlets**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "For the crust:\n",
      "\n",
      "* 1 1/2 cups all-purpose flour\n",
      "* 1/4 cup confectioners' sugar\n",
      "* 1/4 cup unsalted butter, chilled and cut into small pieces\n",
      "* 1/4 cup ice water\n",
      "\n",
      "For the filling:\n",
      "\n",
      "* 1/2 cup fresh spinach, chopped\n",
      "* 2 tablespoons unsalted butter\n",
      "* 2 cloves garlic, minced\n",
      "* 1/2 cup heavy cream\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon black pepper\n",
      "* 1/4 teaspoon paprika\n",
      "* 1 cup large shrimp, peeled and deveined\n",
      "* 1 egg, beaten (for egg wash)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Make the crust:** In a food processor, combine flour, confectioners' sugar, and salt. Add the cold butter and process until the mixture resembles coarse crumbs. Gradually add the ice water, pulsing until the dough comes together in a ball. Wrap and refrigerate for at least 30 minutes.\n",
      "2. **Preheat the oven:** Heat the oven to 400°F (200°C). Line a baking sheet with parchment paper.\n",
      "3. **Roll out the crust:** On a lightly floured surface, roll out the chilled dough to a thickness of about 1/8 inch. Cut out 12 equal-sized squares, about 3 inches (7.5 cm) on each side.\n",
      "4. **Prepare the filling:** In a skillet, melt butter over medium heat. Add garlic and cook for 1 minute. Add the chopped spinach and cook until wilted, about 3-4 minutes. Set aside to cool slightly.\n",
      "5. **Assemble the tartlets:** In a recent bowl, combine the heavy cream, Parmesan cheese, salt, and pepper. Stir to combine.\n",
      "6. **Add the shrimp:** In the bowl with the spinach, stir in the cooked shrimp.\n",
      "7. **Add the cream mixture:** Add the cream mixture to the spinach-shrimp mixture and stir to combine.\n",
      "8. **Assemble the tartlets:** Spoon about 1 tablespoon of the spinach-shrimp mixture onto one half of each dough square, leaving a 1/2-inch border around the edges.\n",
      "9. **Fold and brush the crusts:** Fold the other half of the dough over the filling, pressing the edges to seal. Brush the tops with the beaten egg for a golden glaze.\n",
      "10. **Bake:** Place the tartlets on the prepared baking sheet, about 2 inches (5 cm) apart. Brush the tops with the beaten egg again. Bake for 20-25 minutes, or until the crusts are golden brown and the filling is heated through.\n",
      "11. **Serve:** Serve warm, garnished with fresh spinach leaves and a sprinkle of Parmesan cheese.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* To add extra flavor, sprinkle some chopped fresh herbs (like parsley, chives, or thyme) on top of the tartlets before baking.\n",
      "* If you can't find shrimp, you can substitute with other seafood (like scallops or mussels).\n",
      "* To make ahead, assemble the tartlets and refrigerate for up to 24 hours, or freeze for up to 3 months. Simply thaw and bake when ready.\n",
      "\n",
      "Enjoy your delicious Creamy Spinach and Shrimp Tartlets!\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ____________\n",
    "from langchain_core.messages import ____________\n",
    "\n",
    "# Define API Key and Model Version\n",
    "API_KEY = \"____________________\" \n",
    "MODEL_VERSION = \"____________________\"\n",
    "\n",
    "# Step 0: Connect to NVIDIA API\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION) -> ____________:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API with the specified model.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your NVIDIA API key.\n",
    "        model (str): The model version to use.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance connected to the NVIDIA model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(model=model, api_key=api_key)\n",
    "\n",
    "# Initialize the client\n",
    "client = connect_to_nvidia()\n",
    "\n",
    "# Create a HumanMessage with the prompt\n",
    "prompt = ____________(content=\"Generate a recipe.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.__________([prompt])\n",
    "\n",
    "# Output the response to verify connectivity\n",
    "print(\"Response:\", response.__________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate a Basic Recipe\n",
    "\n",
    "Now that you have a connection to the NVIDIA API, modify the prompt to include a list of ingredients. This will allow the AI to generate a recipe based on specific inputs.\n",
    "\n",
    "**Goal**: Customize the prompt with a list of ingredients and retrieve a recipe that includes them.\n",
    "\n",
    "1. **Complete the Code Below**  \n",
    "   - Define a list of ingredients.\n",
    "   - Use `HumanMessage` to format the prompt with these ingredients for a tailored recipe.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "The model should respond with a recipe that incorporates the specified ingredients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of ingredients\n",
    "ingredients = [__________, __________, __________]  # Example: [\"tomato\", \"basil\", \"olive oil\"]\n",
    "\n",
    "# Create a prompt that includes the ingredients\n",
    "prompt = __________(content=f\"Create a recipe using the following ingredients: {', '.join(__________)}.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.__________([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation\n",
    "print(\"Recipe Response:\", response.__________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Dietary Restrictions\n",
    "\n",
    "Enhance the recipe generation by allowing dietary restrictions. This will enable the AI to create recipes that accommodate specific dietary needs.\n",
    "\n",
    "**Goal**: Modify the prompt to include dietary restrictions, customizing the recipe to align with the specified preferences.\n",
    "\n",
    "1. **Complete the Code Below**  \n",
    "   - Define a list of dietary restrictions.\n",
    "   - Adjust the prompt to include these restrictions.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "The model should respond with a recipe that meets the specified dietary restrictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of ingredients and dietary restrictions\n",
    "ingredients = [__________, __________, __________]  # Example: [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [__________, __________]  # Example: [\"vegan\", \"gluten-free\"]\n",
    "\n",
    "# Create a prompt that includes ingredients and dietary restrictions\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a recipe using the following ingredients: {', '.join(__________)}. Ensure the recipe is {', '.join(__________)}.\"\n",
    ")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.__________([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation with dietary restrictions\n",
    "print(\"Recipe Response with Dietary Restrictions:\", response.__________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Customization with Parameters\n",
    "\n",
    "Enhance recipe generation by adding parameters to control the model's creativity and consistency. Adjusting `temperature` and `top_p` allows you to customize the output style and creativity.\n",
    "\n",
    "**Goal**: Use `temperature` and `top_p` parameters to customize the AI’s response style for recipe generation.\n",
    "\n",
    "1. **Review the Parameter Effects**  \n",
    "   - `temperature`: Controls randomness in the response (higher values make the response more creative, while lower values make it more focused).\n",
    "   - `top_p`: Controls the diversity of words used (higher values allow for more diverse outputs).\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   - Modify `connect_to_nvidia` to accept `temperature` and `top_p` as arguments.\n",
    "   - Use these parameters when creating the `ChatNVIDIA` instance.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this code should produce recipes that vary in style or tone depending on the values of `temperature` and `top_p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the connect_to_nvidia function to accept temperature and top_p\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = __________, top_p: float = __________) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API with configurable temperature and top_p parameters.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your NVIDIA API key.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Controls creativity in the response.\n",
    "        top_p (float): Controls the diversity of words used.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance connected to the NVIDIA model with specified parameters.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(model=model, api_key=api_key, temperature=temperature, top_p=top_p, max_tokens=1024)\n",
    "\n",
    "# Initialize the client with custom temperature and top_p values\n",
    "temperature = __________\n",
    "top_p = __________\n",
    "client = connect_to_nvidia(temperature=__________, top_p=__________)\n",
    "client = connect_to_nvidia(temperature=__________, top_p=__________)\n",
    "\n",
    "# Define a list of ingredients and dietary restrictions\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "\n",
    "# Create a prompt with the ingredients and dietary restrictions\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation\n",
    "print(\"Recipe Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Different Course Types\n",
    "\n",
    "Add a parameter for the type of course (e.g., \"main dish\", \"dessert\", \"appetizer\") to make the recipe more specific. This will help generate recipes tailored to the selected course type.\n",
    "\n",
    "**Goal**: Modify the prompt to include a `course_type` parameter, which allows the model to generate recipes for different types of meals.\n",
    "\n",
    "1. **Define the Course Type**  \n",
    "   Add a variable for the course type, which could be \"main dish\", \"dessert\", or \"appetizer\".\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   - Include `course_type` in the prompt along with ingredients and dietary restrictions.\n",
    "   \n",
    "**Expected Outcome**:  \n",
    "Running this code should produce recipes that are tailored to the specified course type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of ingredients, dietary restrictions, and course type\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "course_type = __________  # Example: \"main dish\"\n",
    "\n",
    "# Create a prompt that includes ingredients, dietary restrictions, and course type\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a {__________} recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\"\n",
    ")\n",
    "\n",
    "# Send the prompt to the model and get the response\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Output the response to verify recipe generation for the specified course type\n",
    "print(\"Recipe Response for Course Type:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation\n",
    "\n",
    "Introduce evaluation metrics to assess the quality and relevance of the generated recipes. We’ll calculate:\n",
    "- **Ingredient Coverage**: Measures how many specified ingredients appear in the recipe.\n",
    "- **Lexical Diversity**: Measures the variety of words used.\n",
    "- **Readability**: Measures how easy the recipe is to read.\n",
    "\n",
    "**Goal**: Implement functions to evaluate the quality of generated recipes based on these metrics.\n",
    "\n",
    "1. **Implement Scoring Functions**  \n",
    "   - `ingredient_coverage_score`: Checks how many ingredients from the list appear in the recipe.\n",
    "   - `lexical_diversity_score`: Calculates the variety of words in the recipe.\n",
    "   - `readability_score`: Calculates the readability of the recipe text.\n",
    "\n",
    "2. **Update the Code Below**  \n",
    "   Implement these functions and use them to evaluate the response generated by the model.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this code should print the evaluation scores for ingredient coverage, lexical diversity, and readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "# Define function to calculate ingredient coverage score\n",
    "def ingredient_coverage_score(recipe: str, ingredients: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the proportion of specified ingredients found in the recipe.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "        ingredients (list): List of ingredients provided in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of specified ingredients found in the recipe text.\n",
    "               Value ranges from 0 to 1, where 1 means all ingredients are present.\n",
    "    \"\"\"\n",
    "    # Convert recipe to lowercase to ensure case-insensitive matching\n",
    "    recipe_lower = __________\n",
    "    \n",
    "    # Count how many ingredients appear in the recipe\n",
    "    ingredient_matches = sum(1 for ingredient in ingredients if ingredient.lower() in __________)\n",
    "    \n",
    "    # Calculate and return the coverage score\n",
    "    return __________ / len(ingredients) if ingredients else 0\n",
    "\n",
    "# Define function to calculate lexical diversity score\n",
    "def lexical_diversity_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical diversity of the recipe text.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Lexical diversity score, which is the ratio of unique words\n",
    "               to total words. A higher score indicates greater word variety.\n",
    "    \"\"\"\n",
    "    # Split the recipe into words\n",
    "    words = recipe.split()\n",
    "    \n",
    "    # Identify unique words and calculate diversity\n",
    "    unique_words = __________\n",
    "    return len(unique_words) / len(__________) if words else 0\n",
    "\n",
    "# Define function to calculate readability score\n",
    "def readability_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the readability score of the recipe using Flesch Reading Ease.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Readability score; higher values indicate easier-to-read text.\n",
    "    \"\"\"\n",
    "    return __________(recipe)\n",
    "\n",
    "# Generate a recipe as in previous steps\n",
    "ingredients = [\"tomato\", \"basil\", \"olive oil\"]\n",
    "dietary_restrictions = [\"vegan\", \"gluten-free\"]\n",
    "course_type = \"main dish\"\n",
    "\n",
    "# Create the prompt with ingredients, dietary restrictions, and course type\n",
    "prompt = HumanMessage(\n",
    "    content=f\"Create a {course_type} recipe using the following ingredients: {', '.join(ingredients)}. Ensure the recipe is {', '.join(dietary_restrictions)}.\"\n",
    ")\n",
    "response = client.invoke([prompt])\n",
    "\n",
    "# Extract recipe text from the model's response\n",
    "recipe_text = response.content\n",
    "\n",
    "# Print the generated recipe\n",
    "print(\"Recipe:\", recipe_text)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "ic_score = ingredient_coverage_score(recipe_text, ingredients)\n",
    "ld_score = lexical_diversity_score(recipe_text)\n",
    "r_score = readability_score(recipe_text)\n",
    "print(\"Ingredient Coverage Score:\", ic_score)  # Measures ingredient inclusion\n",
    "print(\"Lexical Diversity Score:\", ld_score)  # Indicates word variety in the recipe\n",
    "print(\"Readability Score:\", r_score)  # Measures ease of reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Logging to Track Model Interactions and Evaluation Metrics\n",
    "\n",
    "In this step, we’ll introduce logging to keep track of each model interaction, including the prompt, response, model parameters (`temperature` and `top_p`), and evaluation metrics. Logging helps in monitoring, debugging, and analyzing the model's behavior over time.\n",
    "\n",
    "**Goal**: Set up logging to save interaction details for every generated recipe.\n",
    "\n",
    "1. **Configure Logging**  \n",
    "   Set up a basic logging configuration with a file named `llm_ops.log`. Configure it to log messages with timestamps, which will be useful for tracking when each interaction occurred.\n",
    "\n",
    "2. **Define a Logging Function**  \n",
    "   Create a function called `log_llm_interaction` that takes the following parameters:\n",
    "   - `prompt`: The input prompt sent to the LLM.\n",
    "   - `response`: The recipe generated by the LLM.\n",
    "   - `temperature` and `top_p`: Model parameters used for sampling.\n",
    "   - `coverage_score`, `diversity_score`, and `readability`: Evaluation metrics for the generated recipe.\n",
    "\n",
    "   Within this function, use `logging.info` to log each piece of information in a structured format.\n",
    "\n",
    "3. **Log the Interaction**  \n",
    "   After generating and evaluating a recipe, call the `log_llm_interaction` function with the prompt, response, model parameters, and evaluation scores.\n",
    "\n",
    "**Expected Outcome**:  \n",
    "Running this step should save a log entry in `llm_ops.log` with the prompt, model response, parameters, and evaluation metrics for each interaction, enabling you to analyze and troubleshoot model performance effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(__________s - %(message)s\")\n",
    "\n",
    "# Define function to log LLM interactions, including model parameters and evaluation metrics\n",
    "def log_llm_interaction(prompt: str, response: str, temperature: float, top_p: float, coverage_score: float, diversity_score: float, readability: float):\n",
    "    \"\"\"\n",
    "    Logs each interaction with the LLM, including the prompt, response, model parameters, and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the LLM.\n",
    "        response (str): The LLM's response.\n",
    "        temperature (float): Model's temperature parameter used for this interaction.\n",
    "        top_p (float): Model's top_p parameter used for this interaction.\n",
    "        coverage_score (float): Ingredient coverage score.\n",
    "        diversity_score (float): Lexical diversity score.\n",
    "        readability (float): Readability score of the response.\n",
    "    \"\"\"\n",
    "    # Log the prompt\n",
    "    logging.__________(\"Prompt: %s\", __________)\n",
    "    \n",
    "    # Log model parameters\n",
    "    logging.__________(\"Model Parameters: Temperature=%.2f, Top_p=%.2f\", __________, __________)\n",
    "    \n",
    "    # Log evaluation scores\n",
    "    logging.__________(\"Evaluation Scores: Coverage=%.2f, Diversity=%.2f, Readability=%.2f\", __________, __________, __________)\n",
    "    \n",
    "    # Log the response\n",
    "    logging.__________(\"Response: %s\", __________)\n",
    "\n",
    "# Log interaction and evaluation metrics after recipe generation and evaluation\n",
    "log_llm_interaction(__________, __________, __________, __________, __________, __________, __________)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Add Streamlit Interface with Logging\n",
    "\n",
    "In this step, we’ll set up a Streamlit interface for the **AI Chef** recipe generator, allowing users to input recipe preferences and generate recipes with the click of a button. This code also integrates logging to track interactions for analysis and debugging.\n",
    "\n",
    "**Goal**: Create a basic Streamlit interface where users can input recipe parameters, click a \"Generate Recipe\" button, and view the generated recipe.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Copy & Paste**: Copy this code into a new Python file, e.g., `ai_chef_streamlit.py`.\n",
    "2. **Fill in the Blanks**:\n",
    "   - Complete the blanks below to set up the API connection, input fields, and button functionality.\n",
    "3. **Activate Virtual Environment and Install Dependencies**:\n",
    "   - Ensure you have a virtual environment set up and activated.\n",
    "     ```bash\n",
    "     source .venv/bin/activate  # MacOS/Linux\n",
    "     .venv\\Scripts\\activate     # Windows\n",
    "     ```\n",
    "   - Ensure you have all necessary libraries installed, using `requirements.txt`:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "   - If you need to create a `requirements.txt` file, it should include the following:\n",
    "     ```plaintext\n",
    "     streamlit\n",
    "     textstat\n",
    "     langchain_nvidia_ai_endpoints\n",
    "     ```\n",
    "4. **Run the Application**:\n",
    "   - From the terminal, navigate to the directory containing `ai_chef_streamlit.py` and run:\n",
    "     ```bash\n",
    "     streamlit run ai_chef_streamlit.py\n",
    "     ```\n",
    "   - This will open the application in your default browser, enabling you to interact with the recipe generator via the Streamlit interface.\n",
    "\n",
    "**Expected Outcome**: A functional Streamlit app where you can input recipe parameters, click \"Generate Recipe,\" and view the result with evaluation scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import List, Optional\n",
    "import textstat\n",
    "\n",
    "# Set up logging configuration to save interactions in llm_logs.log with timestamps\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define API key and model version (replace 'your_actual_api_key' with your API key)\n",
    "API_KEY = \"your_actual_api_key\"  \n",
    "MODEL_VERSION = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# Connect to NVIDIA API using ChatNVIDIA with specific parameters\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = 0.5, top_p: float = 0.7) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): The API key for NVIDIA access.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Sampling temperature for creativity.\n",
    "        top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance of ChatNVIDIA connected to the specified model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "# Define function to calculate ingredient coverage score\n",
    "def ingredient_coverage_score(recipe: str, ingredients: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the proportion of specified ingredients found in the recipe.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "        ingredients (list): List of ingredients provided in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        float: Proportion of specified ingredients found in the recipe text,\n",
    "               ranging from 0 to 1, where 1 means all ingredients are present.\n",
    "    \"\"\"\n",
    "    recipe_lower = recipe.lower()\n",
    "    ingredient_matches = sum(1 for ingredient in ingredients if ingredient.lower() in recipe_lower)\n",
    "    return ingredient_matches / len(ingredients) if ingredients else 0\n",
    "\n",
    "# Define function to calculate lexical diversity score\n",
    "def lexical_diversity_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical diversity of the recipe text.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Lexical diversity score, which is the ratio of unique words\n",
    "               to total words. A higher score indicates greater word variety.\n",
    "    \"\"\"\n",
    "    words = recipe.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "# Define function to calculate readability score\n",
    "def readability_score(recipe: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the readability score of the recipe using Flesch Reading Ease.\n",
    "    \n",
    "    Args:\n",
    "        recipe (str): The generated recipe text.\n",
    "\n",
    "    Returns:\n",
    "        float: Readability score; higher values indicate easier-to-read text.\n",
    "    \"\"\"\n",
    "    return textstat.flesch_reading_ease(recipe)\n",
    "\n",
    "# Define function to log LLM interactions, including model parameters and evaluation metrics\n",
    "def log_llm_interaction(prompt: str, response: str, temperature: float, top_p: float, coverage_score: float, diversity_score: float, readability: float):\n",
    "    \"\"\"\n",
    "    Logs each interaction with the LLM, including the prompt, response, model parameters, and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the LLM.\n",
    "        response (str): The LLM's response.\n",
    "        temperature (float): Model's temperature parameter used for this interaction.\n",
    "        top_p (float): Model's top_p parameter used for this interaction.\n",
    "        coverage_score (float): Ingredient coverage score.\n",
    "        diversity_score (float): Lexical diversity score.\n",
    "        readability (float): Readability score of the response.\n",
    "    \"\"\"\n",
    "    logging.info(\"Prompt: %s\", prompt)\n",
    "    logging.info(\"Model Parameters: Temperature=%.2f, Top_p=%.2f\", temperature, top_p)\n",
    "    logging.info(\"Evaluation Scores: Coverage=%.2f, Diversity=%.2f, Readability=%.2f\", coverage_score, diversity_score, readability)\n",
    "    logging.info(\"Response: %s\", response)\n",
    "\n",
    "# Main recipe generation function\n",
    "def generate_recipe(client, ingredients: List[str], dietary_restrictions: Optional[List[str]] = None, \n",
    "                    course_type: str = \"main dish\", preference: str = \"easy\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a recipe from the NVIDIA LLM model based on a structured prompt.\n",
    "\n",
    "    Args:\n",
    "        client (ChatNVIDIA): The LLM client instance.\n",
    "        ingredients (List[str]): List of ingredients for the recipe.\n",
    "        dietary_restrictions (Optional[List[str]]): Any dietary restrictions.\n",
    "        course_type (str): Type of course (e.g., main dish).\n",
    "        preference (str): User's recipe preference (e.g., easy).\n",
    "\n",
    "    Returns:\n",
    "        str: Generated recipe text along with evaluation scores.\n",
    "    \"\"\"\n",
    "    # Build prompt\n",
    "    prompt_content = f\"Create a {preference} recipe for a {course_type} using the following ingredients: {', '.join(ingredients)}. \" \\\n",
    "                     f\"Ensure the recipe is {', '.join(dietary_restrictions) if dietary_restrictions else 'no specific dietary restrictions'}.\"\n",
    "    prompt = HumanMessage(content=prompt_content)\n",
    "    \n",
    "    # Send prompt to the model and get response\n",
    "    response = client.invoke([prompt])\n",
    "    recipe_text = response.content\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    coverage_score = ingredient_coverage_score(recipe_text, ingredients)\n",
    "    diversity_score = lexical_diversity_score(recipe_text)\n",
    "    readability = readability_score(recipe_text)\n",
    "\n",
    "    # Log interaction and evaluation metrics\n",
    "    log_llm_interaction(prompt_content, recipe_text, client.temperature, client.top_p, coverage_score, diversity_score, readability)\n",
    "    \n",
    "    return recipe_text, coverage_score, diversity_score, readability\n",
    "\n",
    "# Streamlit UI setup\n",
    "st.title(\"__________\")  # Set title for the Streamlit app\n",
    "\n",
    "# Input fields for recipe customization\n",
    "ingredients = st.__________(\"Enter ingredients (comma-separated):\").split(\", \")  # Text input for ingredients list\n",
    "dietary_restrictions = st.__________(\"Select dietary restrictions\", [\"gluten-free\", \"vegan\", \"vegetarian\", \"dairy-free\"])  # Multi-select for dietary preferences\n",
    "course_type = st.__________(\"Select course type\", [\"Main dish\", \"Dessert\", \"Appetizer\"])  # Dropdown for course type\n",
    "preference = st.__________(\"Select preference\", [\"easy\", \"gourmet\", \"quick\"])  # Dropdown for recipe preference\n",
    "temperature = st.__________(\"Select creativity level (temperature)\", 0.0, 1.0, 0.5)  # Slider to adjust temperature\n",
    "top_p = st.__________(\"Select top_p sampling\", 0.0, 1.0, 0.7)  # Slider for top_p parameter\n",
    "\n",
    "# Button to generate recipe\n",
    "if st.__________(\"Generate Recipe\"):  # Button to trigger recipe generation\n",
    "    # Connect to NVIDIA client with specified temperature and top_p\n",
    "    client = connect_to_nvidia(temperature=temperature, top_p=top_p)\n",
    "    \n",
    "    # Generate recipe based on user inputs and log the interaction\n",
    "    recipe_text, coverage_score, diversity_score, readability = generate_recipe(\n",
    "        client, ingredients, dietary_restrictions, course_type, preference\n",
    "    )\n",
    "\n",
    "    # Display generated recipe and evaluation scores\n",
    "    st.__________(\"### Recipe:\")  # Display recipe section title\n",
    "    st.__________(recipe_text)  # Display generated recipe text\n",
    "    st.__________(\"### Evaluation Scores:\")  # Display scores section title\n",
    "    st.__________(f\"Ingredient Coverage Score: {coverage_score:.2f} (0 to 1 scale)\")  # Display ingredient coverage score\n",
    "    st.__________(f\"Lexical Diversity Score: {diversity_score:.2f} (0 to 1 scale)\")  # Display lexical diversity score\n",
    "    st.__________(f\"Readability Score (Flesch Reading Ease): {readability:.2f} (0 to 100 scale)\")  # Display readability score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing import List, Optional\n",
    "import textstat\n",
    "\n",
    "# Set up logging configuration to save interactions in llm_logs.log with timestamps\n",
    "logging.basicConfig(filename=\"llm_logs.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Define API key and model version (replace 'your_actual_api_key' with your API key)\n",
    "API_KEY = \"your_actual_api_key\"  \n",
    "MODEL_VERSION = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# Initialize conversation memory and recipe history in Streamlit session\n",
    "if 'recipe_history' not in st.session_state:\n",
    "    st.session_state.recipe_history = []  # List to store user and AI messages for chat history\n",
    "\n",
    "# Connect to NVIDIA API using ChatNVIDIA with specified parameters\n",
    "def connect_to_nvidia(api_key: str = API_KEY, model: str = MODEL_VERSION, temperature: float = 0.5, top_p: float = 0.7) -> ChatNVIDIA:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the NVIDIA LLM API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): The API key for NVIDIA access.\n",
    "        model (str): The model version to use.\n",
    "        temperature (float): Sampling temperature for creativity.\n",
    "        top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        ChatNVIDIA: An instance of ChatNVIDIA connected to the specified model.\n",
    "    \"\"\"\n",
    "    return ChatNVIDIA(\n",
    "        model=model,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "# Main recipe generation function with conversation history\n",
    "def generate_recipe(client, ingredients: List[str], dietary_restrictions: Optional[List[str]] = None, \n",
    "                    course_type: str = \"main dish\", preference: str = \"easy\", additional_request: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a recipe from the NVIDIA LLM model based on input and conversation history.\n",
    "\n",
    "    Args:\n",
    "        client (ChatNVIDIA): The LLM client instance.\n",
    "        ingredients (List[str]): List of ingredients for the recipe.\n",
    "        dietary_restrictions (Optional[List[str]]): Any dietary restrictions.\n",
    "        course_type (str): Type of course (e.g., main dish).\n",
    "        preference (str): User's recipe preference (e.g., easy).\n",
    "        additional_request (str): Additional user input or request.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated recipe text along with evaluation scores.\n",
    "    \"\"\"\n",
    "    # Prepare conversation history\n",
    "    conversation_history = [\n",
    "        HumanMessage(content=msg['content']) if msg['role'] == \"__________\" else SystemMessage(content=msg['content'])  # \"user\" for user messages, \"assistant\" for AI responses\n",
    "        for msg in st.session_state.recipe_history\n",
    "    ]\n",
    "\n",
    "    # Build prompt with conversation history\n",
    "    prompt_content = f\"Create a {preference} recipe for a {course_type} using these ingredients: {', '.join(ingredients)}. \"\n",
    "    prompt_content += f\"Ensure the recipe is {', '.join(dietary_restrictions) if dietary_restrictions else 'no specific dietary restrictions'}. {additional_request}\"\n",
    "    conversation_history.append(HumanMessage(content=prompt_content))  # Add the current prompt to the history\n",
    "\n",
    "    # Set up ChatPromptTemplate with conversation history\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful AI Chef assistant creating personalized recipes.\"),\n",
    "        MessagesPlaceholder(variable_name=\"__________\")  # Placeholder for conversation history messages\n",
    "    ])\n",
    "    chain = prompt_template | client\n",
    "    ai_response = chain.invoke({\"messages\": conversation_history}).content  # Get response content\n",
    "\n",
    "    # Save AI response to recipe history\n",
    "    st.session_state.recipe_history.append({\"role\": \"__________\", \"content\": ai_response})  # \"assistant\" role for AI response\n",
    "\n",
    "    # Evaluate and log the recipe\n",
    "    coverage_score = ingredient_coverage_score(ai_response, ingredients)\n",
    "    diversity_score = lexical_diversity_score(ai_response)\n",
    "    readability = readability_score(ai_response)\n",
    "    log_llm_interaction(prompt_content, ai_response, client.temperature, client.top_p, coverage_score, diversity_score, readability)\n",
    "\n",
    "    return ai_response, coverage_score, diversity_score, readability\n",
    "\n",
    "# Streamlit UI setup\n",
    "st.title(\"AI Chef: Your Personalized Recipe Generator\")\n",
    "\n",
    "# Input fields for recipe customization\n",
    "ingredients = st.text_input(\"Enter ingredients (comma-separated):\").split(\", \")\n",
    "dietary_restrictions = st.multiselect(\"Select dietary restrictions\", [\"gluten-free\", \"vegan\", \"vegetarian\", \"dairy-free\"])\n",
    "course_type = st.selectbox(\"Select course type\", [\"Main dish\", \"Dessert\", \"Appetizer\"])\n",
    "preference = st.selectbox(\"Select preference\", [\"easy\", \"gourmet\", \"quick\"])\n",
    "temperature = st.slider(\"Select creativity level (temperature)\", 0.0, 1.0, 0.5)\n",
    "top_p = st.slider(\"Select top_p sampling\", 0.0, 1.0, 0.7)\n",
    "\n",
    "# Initialize ChatNVIDIA client with configured temperature and top_p\n",
    "if \"client\" not in st.session_state:\n",
    "    st.session_state.client = connect_to_nvidia(temperature=temperature, top_p=top_p)\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.recipe_history:\n",
    "    if message[\"role\"] == \"__________\":  # \"user\" for displaying user messages\n",
    "        st.chat_message(\"user\").markdown(f\"**You:** {message['content']}\")\n",
    "    else:\n",
    "        st.chat_message(\"assistant\").markdown(f\"**AI Chef:** {message['content']}\")\n",
    "\n",
    "# User input for additional requests\n",
    "user_input = st.chat_input(\"Enter additional requests or adjustments:\")\n",
    "if user_input:\n",
    "    st.session_state.recipe_history.append({\"role\": \"__________\", \"content\": user_input})  # Add user message to history with \"user\" role\n",
    "    st.chat_message(\"user\").markdown(f\"**You:** {user_input}\")\n",
    "\n",
    "    # Generate recipe with conversation context\n",
    "    with st.spinner():\n",
    "        recipe_text, coverage_score, diversity_score, readability = generate_recipe(\n",
    "            st.session_state.client, ingredients, dietary_restrictions, course_type, preference, user_input\n",
    "        )\n",
    "        st.chat_message(\"assistant\").markdown(f\"**AI Chef:** {recipe_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
